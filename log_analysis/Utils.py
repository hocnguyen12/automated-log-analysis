import itertools
from xml.etree import ElementTree as ET
from pathlib import Path
import json
import streamlit as st

BOLD = '\033[1m'
END = '\033[0m'
############################# PARSE OUTPUT.XML FILE ###################################
def parse_xml(xml_path_string):
    '''
    - input : path to the 'output.xml' file generated by Robot Framework
    - output : parsed xml content that contains the steps of every tests that failed 
               The parsed content can be customized to contain more or less details
               For now, it contains every keyword that is called in the test, with if present, doc and messages, arguments and status
    '''
    xml_path = Path(xml_path_string)
    tree = ET.parse(xml_path)
    root = tree.getroot()

    detailed_failed_tests = []
    for suite in root.iter("suite"):
        for test in suite.findall("test"):
            status = test.find("status")
            if status is not None and status.attrib.get("status") == "FAIL":
                test_name = test.attrib.get("name")
                error_message = status.text.strip() if status.text else "No message"

                doc = test.find("doc")
                doc_text = doc.text.strip() if doc is not None and doc.text else ""

                steps = extract_keywords(test)
                detailed_failed_tests.append({
                    "test_name": test_name,
                    "error_message": error_message,
                    "doc": doc_text,
                    "steps": steps
                })
    return detailed_failed_tests

def extract_keywords(keyword_element, depth=0):
    '''
    Recursive function to extract keyword calls and their arguments, status, doc, and messages.
    '''
    steps = []
    for kw in keyword_element.findall("kw"):
        name = kw.attrib.get("name", "UNKNOWN")
        args = [arg.text for arg in kw.findall("arg") if arg.text]
        status = kw.find("status").attrib.get("status") if kw.find("status") is not None else "UNKNOWN"

        doc = kw.find("doc")
        doc_text = doc.text.strip() if doc is not None and doc.text else ""

        #msgs = [msg.text.strip() for msg in kw.findall("msg") if msg is not None and msg.text]
        msgs = [
            msg.text.strip()
            for msg in kw.findall("msg")
            if msg is not None and msg.text and (msg.attrib.get("level") == "INFO" or msg.attrib.get("level") == "WARN")
        ]

        step = {
            "keyword": name,
            "args": args,
            "status": status,
            "depth": depth,
            "doc": doc_text,
            "messages": msgs
        }
        steps.append(step)
        # Recursively extract nested keywords
        steps.extend(extract_keywords(kw, depth + 1))
    return steps

def pretty_print_fails(fails):
    '''
    for debugging purposes, displays the parsed xml content to easily view what was parsed
    '''
    BOLD = '\033[1m'
    END = '\033[0m'
    for fail in fails:
        print("\n")
        print(BOLD + "Test : " + fail["test_name"] + END)
        print("Error message : " + fail["error_message"])
        print("Doc : " + fail["doc"])
        print("Steps : ")
        if fail["steps"] == [] :
            print("\tEMPTY")
        else :
            for step in fail["steps"]:
                print('\t' + BOLD + "name : " +  step["keyword"] + END)
                print(f"\targs : {step['args']}")
                print(f"\tstatus : {step['status']}")
                print(f"\tdepth : {step['depth']}")
                print(f"\tdoc : {step['doc']}")
                print(f"\tmessage : {step['messages']}")
                print("\n")

def stringify_test_case(test):
    '''
    input : parsed xml fails for A SINGLE fail
    output : string containing all the information about the fail
    '''
    parts = [
        f"Test name: {test['test_name']}",
        f"Error: {test['error_message']}"
    ]

    test_doc = test.get("doc")
    if test_doc:
        parts.append(f"Doc: {test_doc}")

    for step in test["steps"]:
        keyword = step["keyword"]
        args = ", ".join(step["args"])
        status = step["status"]
        doc = step.get("doc", "")
        messages = step.get("messages", [])

        step_parts = [f"Step: {keyword}", f"Args: {args}", f"Status: {status}"]
        if doc:
            step_parts.append(f"Doc: {doc}")
        if messages:
            step_parts.append(f"Messages: {' | '.join(messages)}")

        parts.append(". ".join(step_parts))  # each step becomes a sentence

    return ". ".join(parts)

############################# CONVERT FAILS TO JSON FILE ###################################
def convert_to_json_structured(test):
    '''
    input : output.xml content parsed with XMLlogsParser.parse_xml

    You can then feed this into a:
        - Transformer-based encoder
        - Token classifier
        - Sequence-to-sequence model (to suggest corrections)
    '''
    return {
        "test_name": test["test_name"],
        "error_message": test["error_message"],
        "doc": test["doc"],
        "steps": [
            {
                "keyword": step["keyword"],
                "args": step["args"],
                "status": step["status"],
                "depth": step["depth"],
                "doc": step["doc"],
                "messages": step["messages"]
            }
            for step in test["steps"]
        ]
    }

def save_converted_xml_to_json(xml_file, json_file_name):
    '''
    This function saves the fails to a json file to feed and train the model
    input : path to 'output.xml' file, path to json file that will contain the fails
    output : parsed fail logs
    '''
    fail_logs = parse_xml(xml_file)

    json_data = [convert_to_json_structured(t) for t in fail_logs]

    with open(json_file_name, "w", encoding="utf-8") as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    return fail_logs

def merge_xml_training_data(xml_list, json_file_name):
    '''
    This function does the same treatment as 'save_converted_xml_to_json' but it takes as an input 
    a list of parsed xml contents instead of just one
    The goal is when we have multiple 'output.xml' we want to use as training for the model, 
    we can have a single '.json' that contains all the fails as a dataset
    '''
    training_data = []
    for xml in xml_list:
        fail_logs = parse_xml(xml)
        training_data.extend(fail_logs)

    json_data = [convert_to_json_structured(t) for t in training_data]

    #st.write(training_data)

    with open(json_file_name, "w", encoding="utf-8") as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    return training_data

############################# PROCESS DATA BEFORE FEEDING MODEL ###################################
def build_log_text(item):
    msg = f"Test name: {item['test_name']}\n"
    msg += f"Doc: {item.get('doc', '')}\n"
    msg += f"Error: {item['error_message']}\n"
    for step in item.get("steps", []):
        msg += f"Step: {step['keyword']}\n"
        msg += f"Args: {' '.join(step['args'])}\n"
        msg += f"Status: {step['status']}\n"
        if step.get("doc"):
            msg += f"Doc: {step['doc']}\n"
        if step.get("messages"):
            msg += f"Messages: {' | '.join(step['messages'])}\n"
    return msg

def auto_label_fix_category(data):
    for item in data:
        if "fix_category" not in item or not item["fix_category"]:
            error = item["error_message"].lower()
            if "missing" in error and "argument" in error:
                item["fix_category"] = "missing_argument"
            elif "not found" in error or "selector" in error:
                item["fix_category"] = "invalid_selector"
            elif "assert" in error or "should be equal" in error:
                item["fix_category"] = "assertion_failed"
            elif "timeout" in error:
                item["fix_category"] = "timeout"
            elif "connection" in error:
                item["fix_category"] = "connection_error"
            else:
                item["fix_category"] = "other"
    return data

def auto_label_fix_category_test(data):
    for item in data:
        print(item)
        if "fix_category" not in item or not item["fix_category"]:
            error = item["error_message"].lower()
            if "missing" in error and "argument" in error:
                item["fix_category"] = "missing_argument"
            elif "not found" in error or "selector" in error:
                item["fix_category"] = "invalid_selector"
            elif "assert" in error or "should be equal" in error:
                item["fix_category"] = "assertion_failed"
            elif "timeout" in error:
                item["fix_category"] = "timeout"
            elif "connection" in error:
                item["fix_category"] = "connection_error"
            else:
                item["fix_category"] = "other"
    return data

##################################### LABEL TEST ####################################
if __name__ == '__main__':
    original_data_path = Path("tests/structured_failures_test.json")

    #create list of output.xml files
    list_files = []
    merge_xml_training_data(list_files, original_data_path)

    with open(original_data_path, "r") as f:
        base_data = json.load(f)

    base_data = auto_label_fix_category_test(base_data)

    merged = []

    for item in base_data:
        merged.append({
            "log_text": build_log_text(item),
            "fix_category": item["fix_category"]
        })

    
    # OPTIONAL
    '''
    feedback_entries = []
    if feedback_data_path.exists():
        with open(feedback_data_path, "r") as f:
            feedback_entries = [json.loads(line) for line in f if line.strip()]

    for fb in feedback_entries:
        if fb.get("feedback") == "correct":
            fix_category = fb.get("predicted_category", "unknown") # If the feedback is correct, use the predicted category 
        elif fb.get("feedback") == "wrong":
            fix_category = fb.get("actual_category", "unknown")  # If the feedback is wrong, use the actual category from the user
        else:
            fix_category = "unknown"

        merged.append({
            "log_text": fb["log_text"],
            "fix_category": fix_category
        })
    '''
    # END OPTIONAL

    # Filter out "unknown", "null" and "" fix categories before training
    merged = [entry for entry in merged if entry["fix_category"] not in ["unknown", "null", ""]]

    for entry in merged:
        print(json.dumps(entry, indent=1))

    texts = [r["log_text"] for r in merged]
    labels = [r["fix_category"] for r in merged]
