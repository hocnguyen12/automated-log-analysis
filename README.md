# automated-log-analysis

I am building a project which goal is to analyze robot framework logs.

Robot framework is used to test software, interface, etc and it generates reports after each test execution (output.xml, output.html and report.html)

My goal is to create a system that analyzes a new test execution (generated by robot framework) and suggest corretions/fixes to the fails encountered using machine learning. 

I already trained a model to classify fails, but I need to define clearly what are the target output (classes I want to assign fails to) and have more data to train a more robust model.

Can you help me generate software (maybe an web interface) that is tested using robot framework. generate the interface, and all the robot framework code to test the framework.

I need as many automated tests as possible (maybe not all at once but the goal is to have as manuy as possible), and I need the tests to only run custom keywords (tests cases in .robot files will only call custom keywords so I need to generate many different keywords for each automated test case), the goal is that the tests will all fail

This way I will have many fails  to train my classification model. 

## TODO
- [x] Check that `stringify` adds all the information into the vector/string
- [x] Check if I can add more info about keywords steps in the parsed xml

Tests taken from [robot framework official documentation](https://robotframework.org/?tab=0&example=Advanced%20Example#getting-started)
## Variables

This examples contains multiple tests with different variable types.
${Scalar} Variables

Scalar variables are single instances of a variable. It is used as is. It does not matter if the variable contains a list, object, dictionary or just a string or integer.
@{List-Like} Variables

Lists or list-like variables, which can be iterated, can be "unpacked". A variable containing a list with a $ sign as prefix is still a single variable. A variable containing a list with a @ sign is unpacked and handled as multiple values. You can compare it with a box of 10 cookies. ${box} is just the box, with all the 10 cookies inside. @{box} are 10 cookies without the box. When you hand over @{box} to a keyword, that keyword gets 10 arguments.

Accessing a single element of the box needs the variable as box. So in our case the prefix shall be $. ${box}[1] is the cookie with the index 1.
&{Dictionary} Variables

The & sign as prefix causes that a variable is handled as dictionary or also a list of key=value pairs. Similar to the list variables, dictionary variables are unpacked with the & sign but handle as single variable with the $ sign as prefix.

Accessing a single value by its key, requires in our case the prefix $ again.

    ${dict}[name] accesses the value behind the key "name".
    ${dict.name} is an alternative and sometime handy method to access elements of a dictionary.

## Behaviour-Driven Testing (Gherkin)

This example contains a test case written in a BDD-style with embedded arguments
Test Object

We are testing a fake Calculator implemented in Calculator.py. It provides the functions

    start_calculator
    calculate_term(term)

Test Suite

This test suite (Calculator_Test_Suite.robot) contains a test written in a BDD-style.
The prefixes Given/When/Then/And are dropped when they are matched to the respective keywords in Calc_keywords.resource.
Arguments like "1 + 1" are passed as embedded arguments to improve the readability. Note how the embedded arguments like "${term}" or "${expected}" (in Calc_keywords.resource) are mentioned in the keyword name and later used as variables inside the keyword. Adding quotes around embedded arguments is optional - but a good practice to improve readability.
The Keyword Set Test Variable is used to make a variables available everywhere within the test (as the variable scope is limited to the keyword by default).
 
## Advanced Example 2

This example contains some more advanced features of Robot Framework.
Test Object

We are testing here a backend api for user management. Users must authenticate before interaction. Depending on the authorizations, different actions can be carried out:
    - Administrators can create users, alter user data and fetch details about existing users.
    - Normal users can just fetch their own information and only alter their own details.
    - Guest users can login but not modify anything.

Test Suite

This test suite (test.robot) contains six test cases such as Access All Users With Admin Rights. Test cases are calling either keywords from the resource file keywords.resource or the Library CustomLibrary.py

keywords.resource contains examples of variables, Return-Values, IF/ELSE and FOR-Loops.

# Log and Reports analysis

### Context :

### Steps
- Parse test results (output.xml) and extract:
    Test name
    Steps/keywords executed (in order)
    Final status
    Error message

- Represent this information in a machine-readable format (like vectors or structured objects).

- Analyze similarities across failed tests (based on both what was tested and how).

- Use AI to suggest fixes, based on:
    - Common patterns
    - Prior fixes
    - Context of failure

### Detailed Steps
#### Step 1: Parse Tests with Full Context

Extract:
- Test name
- List of keyword calls (and arguments)
- Failure message
- Failure point (which keyword failed)

Example data structure:
```
{
  "test_name": "Create User",
  "keywords": [
    "Connect    http://localhost    key123",
    "Login User    admin    admin123",
    "Post New User    John    jdoe"
  ],
  "error": "TypeError: TestObject.__init__() missing 1 required positional argument: 'api_key'"
}
```

I can help you write a parser that does this.
#### Step 2: Represent the Test for Machine Learning
**FEATURE ENGINEERING**

You have several options:
- TF-IDF or sentence embeddings on the error + keyword steps as text
- Structured embedding where each keyword is a token or vector
- Use tree-based models or graph-based models (for keyword call hierarchy)

#### Step 3: Cluster Similar Failures

Using:
- Clustering algorithms like KMeans, DBSCAN, etc.
- Or build a nearest neighbor search to find similar failures when a new one happens

#### Step 4: Suggest Fixes

Approaches:
- Associate each cluster with a correction suggestion
- Use past human-written fixes to train a retrieval model or fine-tune a language model
- Rules or heuristics (e.g., if TestObject.__init__ is failing → "Check API key in constructor")

#### Tools & Libraries You’ll Want
- xml.etree.ElementTree or lxml (to parse XML)
- scikit-learn or sentence-transformers (for clustering and embeddings)
- Optionally: faiss or annoy (for similarity search)

## Questions

- ADDING LABELS TO EMBEDDED STRINGS
Thank you it works now, I have more questions :

def extract_keywords(keyword_element, depth=0):
    '''
    Recursive function to extract keyword calls and their arguments
    '''
    steps = []
    for kw in keyword_element.findall("kw"):
        name = kw.attrib.get("name", "UNKNOWN")
        args = [arg.text for arg in kw.findall("arg") if arg.text]
        status = kw.find("status").attrib.get("status") if kw.find("status") is not None else "UNKNOWN"
        step = {
            "name": name,
            "args": args,
            "status": status,
            "depth": depth
        }
        steps.append(step)
        # Recursively extract nested keywords
        steps.extend(extract_keywords(kw, depth + 1))
    return steps

def parse_xml(xml_path_string):
    xml_path = Path(xml_path_string)
    tree = ET.parse(xml_path)
    root = tree.getroot()

    detailed_failed_tests = []
    for suite in root.iter("suite"):
        for test in suite.findall("test"):
            status = test.find("status")
            if status is not None and status.attrib.get("status") == "FAIL":
                test_name = test.attrib.get("name")
                error_message = status.text.strip() if status.text else "No message"
                steps = extract_keywords(test)
                detailed_failed_tests.append({
                    "name": test_name,
                    "error_message": error_message,
                    "steps": steps
                })
    return detailed_failed_tests

def stringify_test_case(test):
    steps_str = " ".join(
        f"{step['name']} {' '.join(step['args'])}" for step in test["steps"]
    )
    return f"{test['name']} {test['error_message']} {steps_str}"

These are the function that parse the xml file for fails ant that stringify the content for the clustering algorithms

but the stringify functions juste puts in a single string the name of the test, the name of the keywords called, the arguments etc. and the ouput looks like this : 

'Test for the year 2022 2025 != 2022 Get Current Date result_format=datetime Log ${date} Should Be Equal As Strings ${date.year} 2022', 'Test Case that fails Sorry. But that was the wrong answer... Bye Bye... Check Correct Greeting Hail Our Robot Overlords! Check Correct Greeting Hello World!'

but how does the algorithms know which part of the string is the name, a keyword name or an argument ? especially with sentence transformers, should I add in the string keywords like "test name : ", "keyword name : ", "args : " etc ? to make it more comprehensible ?

- WHAT ARE THE STEPS FOR ADVENCED/DEEP ML AND SUGGESTIONS OF FIX
I need a little it more clarity about the means I can use to get to the log analyzer that I aim for.

So i have parsed the output.xml file, embedded it into vectors and used clustering algorithms to find similarities and patterns in the logs of failed tests. 

How can I use more advanced techniques to leverage the parsed xml or json content to provide insight about the fails but MOST IMPORTANTLY : provide a smart correction to the fail, which means to smartly give suggestions to modify code or perform other actions to understand where the fail comes from and suggest it. Maybe to do so, providing the entire codebase to the model is needed. If the robot framework tests are testing an interface, maybe also providing the website's DOM would help the model to suggest smart changes to fix the bug

```
Your Final Goal

    A system that analyzes failed Robot Framework tests, understands the failure, and suggests corrections — ideally actionable ones like:

    “Missing api_key in constructor — pass api_key in TestObject(ip, api_key)”

    “Button selector #submit not found — check DOM or ID in the page”

🧭 Track 1: Rule-Based / Heuristic System (fastest to implement)

Use:

    Pattern detection in error messages

    Step patterns (e.g., what failed keyword came after what setup)

    Code/keyword structure in your custom libraries

Examples:

if "TypeError" in error and "api_key" in error:
    suggestion = "Check if you forgot to pass 'api_key' in your CustomLibrary or connect()"

Benefits:

    Fully offline

    Explainable and testable

    Starts producing value quickly

🧠 Track 2: Code-Aware ML Models

You train a classifier or model that:

    Takes as input: test steps + error message + keyword arguments

    Outputs: a probable category of failure (e.g., "missing argument", "bad selector", "backend error")

Then each category is mapped to suggested fix logic.

You can use:

    scikit-learn or XGBoost for structured features

    Small local LLMs like CodeGen, StarCoder, Phi, or TinyLlama if you go deep

What you'd need:

    A training dataset of input = {error + steps}, label = fix_type

    Features from code, steps, and maybe logs

    A curated list of fix types

🤖 Track 3: Use a Small Local Language Model (LLM) for Code Reasoning

You give the model:

    Test failure (output.xml)

    JSON structure of steps

    Snippets of the code being tested (e.g. keywords, libraries, frontend or backend)

    Optionally, HTML or page structure (DOM)

The model can:

    Infer what’s wrong

    Suggest natural language explanations

    Suggest code diffs or code inserts to fix

Offline LLM tools (local):

    llama.cpp with models like Phi-2, CodeLlama-Instruct

    Ollama to run models locally

    LM Studio for chatting with local models

🧩 Example Multi-Modal Fix Suggestion Architecture

                    ┌────────────────────────────┐
                    │ Robot output.xml parser    │
                    └────────────┬───────────────┘
                                 ↓
                ┌──────────── Structured JSON ────────────┐
                │ test_name, steps, args, error, doc, msg │
                └────────────┬────────────────────────────┘
                             ↓
       ┌──────────── Embed or tokenize steps/error ─────────────┐
       ↓                                                        ↓
 Rule-based corrections                                Cluster → Fix mapping
 (e.g. api_key missing → inject arg)        or         (cluster X → fix Y)

       ↓                                                        ↓
        ┌────────────────────────────────────────────────────┐
        │ Suggest fix: code snippet / doc / test to re-run   │
        └────────────────────────────────────────────────────┘

🧠 Optional Inputs for Deeper Fixing

If your tests target a website or app:

    DOM snapshots or HTML dump of the tested page

    Screenshot (visual analysis — harder, but feasible)

    API specs or schema of the backend being tested

If your tests rely on custom Python keywords:

    Parse your CustomLibrary.py

    Include docstrings, argument signatures

    Link failure to actual code line (if traceback is available)

🔧 Next Steps for You (based on your goal)
✅ Short-Term:

    Add rules for common errors you found (e.g., TypeError, SelectorNotFound, etc.)

    Map each cluster to a suggested fix template

    Try embedding CustomLibrary.py function headers into the same vector space as failures

🧪 Mid-Term:

    Create a dataset of (failure + fix) examples

    Train a classifier to predict fix types

    Try prompting a small LLM like phi2 or codellama locally with test_json + code snippet → fix

🧠 Long-Term:

    Build a feedback loop: when a fix works, reinforce it

    Track regressions and compare output.xml over time

    Build a local assistant UI for reviewing suggestions
```

- SAME QUESTION
As you can see, the output.xml file gives every steps of every keywords, if there is a FOR loop written in robot framework, we will see every iterations in the xml file, can this be used in a more advanced model to give fix suggestions ? 

Can it be used in my clustering algorithms to enhance classification or is it too much detail and maybe it will confuse the clustering algorithm ?


I especially want to go to the next step and start working on suggesting fix, I dont want to implement a rule based algorithm because it would take too long considering the size of the logs I want to analyze in the future.

Therefore, I would like to start working on solutions like code-aware ML models (that would know the codebase, so the robot framework tests, keywords and variables and the website DOM that we are testing) or even a small local LLM that would also know about these same inputs. 

What are detailed steps that would help me go in this direction ?

### Project Landmark
```
So now before starting fine tuning codellama or t5, I just want to make some things a bit clearer :

- Starting from the output.xml file, I now have homemade python libraries that parse the xml file to retrieve the fails and relevant informations like error massage, doc, keywords called, etc.

- I also have a way to stringify those fails and embed them for clustering algorithms, I have a python script that runs several clustering algorithms, kmeans, dbscan, hdbscan, spectral, etc to compare the way fails are clustered and fine tune the parameters for a more relevant clustering

- On the other hand I have the notebook we made, which takes the SAME fails, extracted from the xml with the same function, converted into json and then classify them, This means that for any new fail, a fix will be suggested by looking at previous fails that had similarities and suggesting the same fix.

unlike for simple clustering, this part needs a special effort, to label the fails by hand or with rules, so that for each fail that the model trains on, it knows exactly what is the fix for this fail, and it will suggest the same fix to similar fails

On top of this, thanks to FAISS, I can find the n (for now n=3) most similar fails and display them to the user so that he can look if the suggested fix is relevant and look at what worked for these previous fails

My questions :

- With the second method (classification), can the model learn with each new fail it encounter if we tell the model if the suggested fail was succesful or not ?

- Now the clustering only allows to visualize how fails are similar to each other, but can we leverage the clusters to maybe feed a more advanced algorithm that will suggest a similar fix to fails of the same cluster ?

Just to remember, the goal of the project is to have a system that analyses fail logs from automated tests, that testers can use everyday to fix bugs faster, if the system can add the new fails it encounters to its knowledge base, with the fix that actually worked (with input from the user after he corrected the bug), that would be better
```
## Using a small local LLM
### Ollama
`https://ollama.com/`

- Install : 
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

```bash
pip install ollama
```

- Test model in terminal : 
```bash
ollama run phi
# or for code-specific model
ollama run codellama:7b-instruct
```

- test model with python api (automatic prompt building) :
```bash
python3 log_analysis/ollamaLM.py
```

## Building a small ui
```bash
pip install streamlit
```
